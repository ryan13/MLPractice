{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customer segementation plays an important role to identify which group a company should focus on in retention by promotion, which group is a main source of company's profit. There are several techniques including K Means Clustering, EM (Expectation Maximization). While K Means are commonly, it suffers a drawback. The algorithm works well when a boundary between clusters is clearly determined. **In other word**, one data point should belongs to one cluster or not straddle in seperated line. K-Means uses Euclidean metrics that illustrates well a sphere boundary with a centered centroid\n",
    "\n",
    "Real world data may hardly be perfect this way. EM considers the data point belongs to one cluster if the probability of its memebership is maximum. This means EM computes the probability of membership for any data point belongs to each of clusters. In this notebook, we explorehow to use EM approach to segment music (Jazz) into categories. We adapt from [source](\"https://www.google.com/search?ei=kVoLWoWvH8LqjwPUjL6QBA&q=thoughtful+machine+learning+with+python&oq=thoughtful+machine+learning+with+python&gs_l=psy-ab.3..35i39k1j0i67k1l2j0l6.1126.2246.0.2668.8.8.0.0.0.0.117.809.4j4.8.0....0...1.1.64.psy-ab..0.8.805....0.CnGd_QoiG0A\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import random\n",
    "import logging\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import LinAlgError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import random\n",
    "import logging\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import LinAlgError\n",
    "\n",
    "\n",
    "def dvmnorm(x, mean, covariance, log=False):\n",
    "    \"\"\"density function for the multivariate normal distribution\n",
    "    based on sources of R library 'mvtnorm'\n",
    "    :rtype : np.array\n",
    "    :param x: vector or matrix of quantiles. If x is a matrix, each row is taken to be a quantile\n",
    "    :param mean: mean vector, np.array\n",
    "    :param covariance: covariance matrix, np.array\n",
    "    :param log: if True, densities d are given as log(d), default is False\n",
    "    \"\"\"\n",
    "  # TODO: add another methods from mvtnorm (calculate matrix square root using eigenvalues or SVD\n",
    "  # TODO: add check for matching of input matrix dimensions\n",
    "    n = covariance.shape[0]\n",
    "    try:\n",
    "        dec = np.linalg.cholesky(covariance)\n",
    "    except LinAlgError:\n",
    "        dec = np.linalg.cholesky(covariance + np.eye(covariance.shape[0]) * 0.0001)\n",
    "    tmp = np.linalg.solve(dec, np.transpose(x - mean))\n",
    "    rss = np.sum(tmp * tmp, axis=0)\n",
    "    logretval = - np.sum(np.log(np.diag(dec))) - 0.5 * n * np.log(2 * math.pi) - 0.5 * rss\n",
    "    if log:\n",
    "        return logretval\n",
    "    else:\n",
    "        return np.exp(logretval)\n",
    "\n",
    "\n",
    "class EMClustering(object):\n",
    "    logger = logging.getLogger(__name__)\n",
    "    ch = logging.StreamHandler()\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    ch.setFormatter(formatter)\n",
    "    logger.addHandler(ch)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    cluster = namedtuple('cluster', 'weight, mean, covariance')\n",
    "\n",
    "    def __init__(self, n_clusters):\n",
    "        self._data = None\n",
    "        self._clusters = None\n",
    "        self._membership_weights = None\n",
    "        self._partitions = None\n",
    "        self._n_clusters = n_clusters\n",
    "\n",
    "    @property\n",
    "    def partitions(self):\n",
    "        return self._partitions\n",
    "\n",
    "    @property\n",
    "    def data(self):\n",
    "        return self._data\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        return self._membership_weights\n",
    "\n",
    "    @property\n",
    "    def clusters(self):\n",
    "        return self._clusters\n",
    "\n",
    "    def fit_predict(self, data, iteration=5):\n",
    "        self.setup(data)\n",
    "        for i in range(iteration):\n",
    "            self.logger.debug('Iteration %d', i)\n",
    "            self.expect()\n",
    "            self.maximize()\n",
    "        return self\n",
    "\n",
    "    def setup(self, data):\n",
    "        self._n_samples, self._n_features = data.shape\n",
    "        self._data = data\n",
    "        self._membership_weights = np.ones((self._n_samples, self._n_clusters)) / self._n_clusters\n",
    "        self._s = 0.2\n",
    "\n",
    "        indices = list(range(data.shape[0]))\n",
    "        random.shuffle(indices)\n",
    "        pick_k_random_indices = random.sample(indices, self._n_clusters)\n",
    "\n",
    "        self._clusters = []\n",
    "        for cluster_num in range(self._n_clusters):\n",
    "            mean = data[pick_k_random_indices[cluster_num], :]\n",
    "            covariance = self._s * np.eye(self._n_features)\n",
    "            self._clusters.append(self.cluster(1.0 / self._n_clusters, mean, covariance))\n",
    "\n",
    "        self._partitions = np.empty(self._n_samples, dtype=np.int32)\n",
    "\n",
    "    def expect(self):\n",
    "        log_likelyhood = 0\n",
    "        for cluster_num, cluster in enumerate(self._clusters):\n",
    "            log_density = dvmnorm(self._data, cluster.mean, cluster.covariance, log=True)\n",
    "            membership_weights = cluster.weight * np.exp(log_density)\n",
    "            log_likelyhood += sum(log_density * self._membership_weights[:, cluster_num])\n",
    "\n",
    "            self._membership_weights[:, cluster_num] = membership_weights\n",
    "\n",
    "        for sample_num, probabilities in enumerate(self._membership_weights):\n",
    "            prob_sum = sum(probabilities)\n",
    "\n",
    "            self._partitions[sample_num] = np.argmax(probabilities)\n",
    "\n",
    "            if prob_sum == 0:\n",
    "                self._membership_weights[sample_num, :] = np.ones_like(probabilities) / self._n_clusters\n",
    "            else:\n",
    "                self._membership_weights[sample_num, :] = probabilities / prob_sum\n",
    "\n",
    "        self.logger.debug('log likelyhood %f', log_likelyhood)\n",
    "\n",
    "    def maximize(self):\n",
    "        for cluster_num, cluster in enumerate(self._clusters):\n",
    "            weights = self._membership_weights[:, cluster_num]\n",
    "\n",
    "            weight = np.average(weights)\n",
    "            mean = np.average(self._data, axis=0, weights=weights)\n",
    "            covariance = np.cov(self._data, rowvar=False, ddof=0, aweights=weights)\n",
    "\n",
    "            self._clusters[cluster_num] = self.cluster(weight, mean, covariance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "# In case the above code stored in em_clustering.py, uncomment the following\n",
    "#from em_clustering import EMClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "artists = []\n",
    "years = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Read data from csv with open, Do Not use\n",
    "with open('C:/dataset/annotated_jazz_albums.csv', 'r') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    headers = reader.fieldnames[3:]\n",
    "    for row in reader:\n",
    "        artists.append(row['artist_album'])\n",
    "        years.append(row['year'])\n",
    "        data.append([int(row[key]) for key in headers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1378"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Alternative, we can do \n",
    "# assert data= df.iloc[:,3:].values\n",
    "data= df.iloc[:,3:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist_album</th>\n",
       "      <th>key_index</th>\n",
       "      <th>year</th>\n",
       "      <th>Aboriginal</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Acid Jazz</th>\n",
       "      <th>Acoustic</th>\n",
       "      <th>African</th>\n",
       "      <th>Afro-Cuban</th>\n",
       "      <th>Afro-Cuban Jazz</th>\n",
       "      <th>...</th>\n",
       "      <th>Swing</th>\n",
       "      <th>Synth-pop</th>\n",
       "      <th>Tech House</th>\n",
       "      <th>Techno</th>\n",
       "      <th>Theme</th>\n",
       "      <th>Therapy</th>\n",
       "      <th>Thrash</th>\n",
       "      <th>Trance</th>\n",
       "      <th>Trip Hop</th>\n",
       "      <th>Vocal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Duke Ellington In A Mellotone</td>\n",
       "      <td>0</td>\n",
       "      <td>1940</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Duke Ellington Sophisticated Lady</td>\n",
       "      <td>1</td>\n",
       "      <td>1940</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Duke Ellington Black Brown and Beige</td>\n",
       "      <td>2</td>\n",
       "      <td>1943</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Coleman Hawkins Rainbow Mist</td>\n",
       "      <td>3</td>\n",
       "      <td>1944</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mary Lou Williams Zodiac Suite</td>\n",
       "      <td>4</td>\n",
       "      <td>1945</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           artist_album  key_index  year  Aboriginal  \\\n",
       "0         Duke Ellington In A Mellotone          0  1940           0   \n",
       "1     Duke Ellington Sophisticated Lady          1  1940           0   \n",
       "2  Duke Ellington Black Brown and Beige          2  1943           0   \n",
       "3          Coleman Hawkins Rainbow Mist          3  1944           0   \n",
       "4        Mary Lou Williams Zodiac Suite          4  1945           0   \n",
       "\n",
       "   Abstract  Acid Jazz  Acoustic  African  Afro-Cuban  Afro-Cuban Jazz  ...    \\\n",
       "0         0          0         0        0           0                0  ...     \n",
       "1         0          0         0        0           0                0  ...     \n",
       "2         0          0         0        0           0                0  ...     \n",
       "3         0          0         0        0           0                0  ...     \n",
       "4         0          0         0        0           0                0  ...     \n",
       "\n",
       "   Swing  Synth-pop  Tech House  Techno  Theme  Therapy  Thrash  Trance  \\\n",
       "0      1          0           0       0      0        0       0       0   \n",
       "1      1          0           0       0      0        0       0       0   \n",
       "2      0          0           0       0      0        0       0       0   \n",
       "3      1          0           0       0      0        0       0       0   \n",
       "4      0          0           0       0      0        0       0       0   \n",
       "\n",
       "   Trip Hop  Vocal  \n",
       "0         0      0  \n",
       "1         0      0  \n",
       "2         0      0  \n",
       "3         0      0  \n",
       "4         0      0  \n",
       "\n",
       "[5 rows x 130 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we use pandas \n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/annotated_jazz_albums.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1378, 130)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Alternative, we can do \n",
    "# assert data= df.iloc[:,3:].values\n",
    "data= df.iloc[:,3:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-14 12:48:20,375 - __main__ - DEBUG - Iteration 0\n",
      "2017-11-14 12:48:22,965 - __main__ - DEBUG - log likelyhood -30145.611290\n",
      "2017-11-14 12:48:23,245 - __main__ - DEBUG - Iteration 1\n",
      "2017-11-14 12:48:23,465 - __main__ - DEBUG - log likelyhood 415701.263098\n",
      "2017-11-14 12:48:23,838 - __main__ - DEBUG - Iteration 2\n",
      "2017-11-14 12:48:24,134 - __main__ - DEBUG - log likelyhood 592838.564443\n",
      "2017-11-14 12:48:24,307 - __main__ - DEBUG - Iteration 3\n",
      "2017-11-14 12:48:24,526 - __main__ - DEBUG - log likelyhood 600303.866821\n",
      "2017-11-14 12:48:24,697 - __main__ - DEBUG - Iteration 4\n",
      "2017-11-14 12:48:24,915 - __main__ - DEBUG - log likelyhood 602965.110886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  2 13  2 13  4  2  4 13  4 13 13  4  4 23  4  4  4  2 13  4  2 13 13  4\n",
      "  4 13  4  2  4  4 13  8 13  2 13  4  4  4 13 13 13  4 13 13 13 13  4  4 13\n",
      "  2 21 13  4  4  4  4  4  4  4  3  4 13  4 13  4  4 13  4  4 13 23  4 13  4\n",
      " 13 23  4 13  4  4 13  4  3 13 13  4 13 13 14  3  4  4 13  4 13  4  4  4 13\n",
      " 13  4  4  4 21 13 13 21 13 13 23  4 13  4 13  4 13 13 13  4  4  4 23 10 13\n",
      "  4  4 13  6  4 21  4 13  4 16 13 13 13 13  4 13  4  4 13  4  4 23  4  4 23\n",
      "  2  4 13 14  1 13 14 13 13 13  4  4  4 13 13 13 13 13  2 13  3  3 13 13  4\n",
      " 23 13  4  4 13  4  4 23  4  4 13 23 13 23 13  8  4 13 13 13 13 13 13  4  4\n",
      "  4 13 15  2  9 13  4  4  4 13  3 13 13  4 13 13 13  4  3 13  4 13 13 13 13\n",
      " 13  4 13 13 13 16 13 13  8  8  4 23 13  4 13 13  4 13  4 23 13  3  3 13 13\n",
      " 13 13 13 13 13 13  2  4 23 16 13 13 13 13 13 13 13  3  4 13 23 13  4 23  4\n",
      " 13 13  8 23 13 14 11 13 13  4 13 13 13 13 13 13  4 13 13 13  4 13 13  8 13\n",
      " 13 13 13  8 13 13 13 13  4 13 23 13 21  8 13 13  4 13  4  4 13 13 13  0 23\n",
      " 11  8  8 13 16 13 13 23 13 23 13 13 13  4 13 13 13  4 13 13 13 13 13  8 13\n",
      " 13 23 13  0 13 14  5 23 13  5  4 13 16 13 23  4  8  1 13 13  8 23  0 13 23\n",
      " 13  4  4 13  1 13  1  5 13 13  1 13 13 13 13 13 13  2 14 23 13 13 13 13 13\n",
      " 16  8 14 13 13  0 14 13  8 13 13 13 13  0 13  1  1 13  4 23  4  1  1 13 13\n",
      " 13  0  0 13  1  8 21 13 21  8 13 13  1 21 13  4 13 13 13 13  5 13 13 13 13\n",
      "  4  8  4  5 13  1  1  1  0 21 23 13 13 13 13 13 13 13  4 13 13  5 13 21 23\n",
      "  4 13  4 13  4 13  0  4 13  1 13 13 13  4  1  3 13  0  0  1  0 13 13 13 23\n",
      " 13  1  0 13 13 13  4  1 13 13 13 23  5  4  1 13 13 13 13 23 13  0 13 13  2\n",
      "  5 13  4 13  1 13 13  0 13  0  3  8  4  4 13  0 13 13 13  2  8 13 13 13  4\n",
      " 13  1 13  9  4 13  4  4  2 14 16 13 13 13  3 14 13  1  4  2  4 13 23 13 13\n",
      "  4 13 13 13 13  6  1  4 13  4  0 13  5 13  4  1  4 13 13  0 13  0 13  8 13\n",
      " 13 13 13 13 13 23  3  8  1  0 13 16  4 13 13 13  4 13 13  8  8 13  4  8 13\n",
      " 13 13  0 13 13 13  4 13  4 13  4 23 13 13 13  4 13  8 13  4 13  1 13  8  8\n",
      " 13  9 13 13  1 13 13 13 13 13 13 13 13 13 13 16 13 13  8 16 13 13 13  4 11\n",
      " 23 13 13  0  8 13 13 13  8  8 23 13 13  4  2 13 13 13 13 13  4  8 13 13 13\n",
      " 13  6  4  1  8 13 13  4 13 13  4 13 13  4 13 14 13  4  8 13  4  4 13  4 13\n",
      " 16 23  1 10 13 13  0 13 13 23 13 13  1 13 13  4  5 13 13  8 13 13 23 13 13\n",
      "  3 13 13  4 13  4 13  3 13 13  1 13  4  8 13 13 23 13 13  4 13 13  4 13 13\n",
      " 13  8 13 13 13 13 13  2  1 23 13  3  4  4 13 14 11  0  4 13 13  4  8  8  0\n",
      "  4  4  8  4 13  3 13  6 13 13 13  3 13  1 13  3  3  1  4 13  4 13 13 13 16\n",
      " 23  1 13  8  4 13 13  1 13 16  4 13  4 13 13  4  4  1 13  4 13  4  4  4  0\n",
      " 16  4  1  8 13 13 13 13 13 13 13  4 13  4  3 13  4 16  4 13 13  4  8 13 13\n",
      "  0 13  4 21  3 13 13  4 13  1  4 13  4  4  3 16  4  5  2 16 13 13  6 13 13\n",
      " 13 13 13 13  0  0 13 13 13  2 13 13  4  3  8 13 13 13 21  4  4 13  0  1 13\n",
      " 11 14  3 13 16  4  2  4 13  1  4 13  3 11  4  4  1 13  8 13 13  3  4  4 13\n",
      "  1  4 13 13  1 13  8 13 13  0  4  4 13  4 13  4  4  1 13 13 13 13  4 13 13\n",
      " 16 13  4  4 13 13  3  3 13 13 13 13  6 13 13 16 13 13 11  4 13  4 13 13 13\n",
      "  4  4 13 13 13  9  2 13 13 13 13 13 23  4  8 13 13 13 13 13  4 13 13 13 13\n",
      " 13 13 13  4 13 13  2  4 13 13 13 13  4 13 13 13 13 14  4 13 13  8  3 13  4\n",
      " 13  4  8 13 13 13  2  3 14 13 13  4 13 21  4 13 13 13 13 13 13 13  4 13  3\n",
      "  4 13 13 13 23  8  2  4 13 13  0  4 13  4 13 13 13 13  4 14  4 13  4 13 13\n",
      " 13 13  4 13  4 13 13 13 13 13 13  3 13 11 13 13  8 13 13 13 13 13  3 13  4\n",
      " 13 13 13 13 13 13 13 13  4 13 13 13 13 13  3 13  4 13 13 13 13 13 14 13 13\n",
      " 14 13 23 13 13 13 13 13 13 13 13 13 13 13 13 13 13  3  3 13 13 13 13 13 13\n",
      "  4  4 13 13 13 13  2 11  4 13 13 13 13 11 13 13 13 13 13 13  3 13 13 13 13\n",
      " 13 13 13  8 13 13  4 13 13 13 13 13 13 13 13 13 14  4 13 13 13 13 13 13 13\n",
      " 13 13  4 13  4 13 13 11 13 13 13  3 10 13  0 13 13 13 13 13 13 13 13 13 13\n",
      " 13  4  3 13 13 13 13  8  4  3 13  4 13  4 13 13 13 13 13 13 13 13 13  2 13\n",
      " 13 13 13 13 13 13  8 13  1 13 13 13 13  4 13 13 13 13 13 13 13 13 13 13 13\n",
      " 13 13 13  0 13 13 11 13 13 13 13 13  4  4  4 14 13 13  4 13 13 13 13  3 13\n",
      " 13 13  4 13 13 13  4 13  4 13 13  4 23 13 13 13 13 13 13 13 13 13 13 13 13\n",
      " 13  4  4 13 13 13 13 13  4 13 13  3 13 13 13 13 13  4  0 13 13 13 13 13 13\n",
      " 23 13  4]\n"
     ]
    }
   ],
   "source": [
    "clusterer = EMClustering(n_clusters=25)\n",
    "clusters = clusterer.fit_predict(np.array(data))\n",
    "\n",
    "print(clusters.partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is a drawback, each run we may get different clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "clusters = KMeans(n_clusters=25).fit_predict(data)\n",
    "\n",
    "\n",
    "with open('clustered_kmeans.csv', 'w') as csvfile:\n",
    "    fieldnames = ['artist_album', 'year', 'cluster']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        writer.writerow({'artist_album': artists[i], 'year': years[i],'cluster': cluster})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
