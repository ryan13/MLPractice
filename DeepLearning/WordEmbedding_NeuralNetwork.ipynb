{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings:\n",
    "It is an approach to define a dense vector representation of words that captures something about their meaning instead of bag-of-word. The position of the word within the vector space is learned from text and is based on the words tath surround the word when it is used. Simply put, it illustrates how a set of fixed length dense and continuos-valued vectors is trained from a large corpus of text. **gensim** python library provides built-in function for such task. Two common methods are Word2Vec and Glove."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides an **Embedding** layer for neural network on text data where input data is encoded as integer s.t. each word represents by a unique integer. It is the first layer of a neural network that consists of:   \n",
    "* **input_dim**: This is the size of the vocabulary in the textdata   \n",
    "* **output_dim**: This is the size of the vector space where words willbe embedded (size of the output vectors from this layer for each word).   \n",
    "* **input_length**: This is a length of input sequences. E.g, if all of your input docs consists of 1000, input_length=1000.   \n",
    "\n",
    "For example: we define an **Embedding** layer with a vocabulary of 200 (e.g, integer encoded wod from 0 to 199), a vector spae of 32 dimensions in which words will be embedded, and input documents with 50 words each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.embeddings import Embedding\n",
    "e = Embedding(200,32,input_length=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of Embedding layer is a 2D vector with one embedding for each word in the input sequences of words (input document). As a result, to connect a Dense layer directly to an Embedding layer, we need flatten the 2D output matrix to 1D vector using the **Flattern** layer. Let bring the codes together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let begin with a tiny collection of 10 text documents which defines a comment aboustudents' work. Each text document is classified as positive 1 or negative 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs =[ 'Well done!', 'Good work', 'Great effort', 'nice work','Excellent!','Weak',\n",
    "       'Poor effort','not good','poor work','Could have done better.']\n",
    "labels = np.array([1,1,1,1,1,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to encode each document so that, the **Embedding** layer will have sequences of input integers. We use vocabulary size of 50 to reduce a prbability of collisions from the hash function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16, 1], [28, 38], [32, 33], [38, 38], [40], [7], [34, 33], [31, 28], [34, 38], [17, 9, 1, 22]]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50\n",
    "encoded_docs = [one_hot(d,vocab_size) for d in docs]\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequences have different lengths which Keras requires to vectorize in fixed length. This is known as padding. Assume that we want all input sequences of lenght 4 with **pad_sequence()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16  1  0  0]\n",
      " [28 38  0  0]\n",
      " [32 33  0  0]\n",
      " [38 38  0  0]\n",
      " [40  0  0  0]\n",
      " [ 7  0  0  0]\n",
      " [34 33  0  0]\n",
      " [31 28  0  0]\n",
      " [34 38  0  0]\n",
      " [17  9  1 22]]\n"
     ]
    }
   ],
   "source": [
    "max_length =4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen= max_length, padding='post')\n",
    "print(padded_docs)                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding layer has a vocabulary of 50 and an input length of 4. The output from Embedding layer will be 4 vectors of  8 dimensions each, one for each word; then we flatten this to a one 32 element vector to pass to the **Dense** output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 4, 8)              400       \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 433\n",
      "Trainable params: 433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    model =Sequential()\n",
    "    model.add(Embedding(vocab_size,8,input_length = max_length))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics =['acc'])\n",
    "    model.summary()\n",
    "    model.fit(padded_docs,labels,epochs=50,verbose = 0)\n",
    "    return model\n",
    "\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.000001\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(padded_docs,labels,verbose=0)\n",
    "print('Accuracy: %f'% (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pre-Trained Glove Embedding   \n",
    "Keras Embedding layer can us a word embedding. For example, we can download pre-trained GloVe with smallest size, **glove.6B.zip** which includes 50, 100, 200 and 300 dimensions. Interested can refer to example: **pretrained_word_embeddings.py**, example downloaded file is **glove.6B.100d.txt** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load pretrainde Glove\n",
    "embeddings_index =dict()\n",
    "f =open('glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values =line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:],dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty slow. It is better to filter the embedding for the unique words in the training data. We need to create a matrix of one embedding for each word in training set by enumerating all unique words in the **Tokenizer.word_index** and locating the embedding weight vector from the loaded Glove embedding to generate a matrix of weigths only for words in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a weight matrix for words \n",
    "embedding_matrix = zeros((vocab_size,100))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following **Embedding** layer  uses 100-dimensional version where output_dim =100. To avoid updating the learned word weights, we set **trainable** =False in creating model. The full example is follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Embedding\n",
    "\n",
    "docs = ['Well done!','Good work','Greate effort','nice work','Excellent!','Weak',\n",
    "        'Poor effort!','not good','poor work','Could have done better.']\n",
    "labels = np.array[1,1,1,1,1,0,0,0,0,0]\n",
    "\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "vocab_size = len(t.word_index) +1\n",
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "print(encoded_docs)\n",
    "\n",
    "# padding docs to a max length of 4 words\n",
    "max_length =4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen = max_length, padding='post')\n",
    "print(padded_docs)\n",
    "\n",
    "\n",
    "# load the whole embedding \n",
    "embeddings_index = dict()\n",
    "f = open('glove.6B.100d.txt', mode='rt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:],dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors'% len(embeddings_index))\n",
    "\n",
    "\n",
    "# create a weight matrix for words in training\n",
    "embedding_matrix = zeros((vocab_size,100))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    is embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "# create model\n",
    "def create_model():\n",
    "    model =Sequential()\n",
    "    e = Embedding(vocab_size,100, weights=[embedding_matrix],input_length=4, trainable = False)\n",
    "    model.add(e)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    # compile\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['acc'])\n",
    "    model.summary()\n",
    "    # fit model\n",
    "    model.fit(padded_docs,labels,epochs=50,verbose=0)\n",
    "    return model\n",
    "\n",
    "loss, accuracy = model.evaluate(padded_docs,labels, verbose=0)\n",
    "print('Accuracy %f'% (accuracy*100))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
