{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text classification involves the use of word embedding for representing words and a Convolutional Neural Netword (CNN). The model can be described as following:   \n",
    "* **Word Embedding** A distributed representation of words where different words that have a similar meaning (based on their usage) also have a similar representation.   \n",
    "* **Convolutional Model**: A feature extraction model that learns to extract salient feature from documents represented using a word embedding.   \n",
    "* **Fully connected model**: The intepretation of extracted features in term of a predictive output.  \n",
    "\n",
    "Initial step, we do some preprocessing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44276\n",
      "44276\n"
     ]
    }
   ],
   "source": [
    "# load doc into mem\n",
    "def load_doc(filename):\n",
    "    file = open(filename,'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# generate token from a doc\n",
    "def clean_doc(doc):\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]'% re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('',w) for w in tokens]\n",
    "    # remove non alphabetic token\n",
    "    tokens =[word for word in tokens if word.isalpha()]\n",
    "\n",
    "    # filter out stop words\n",
    "    stop_words =set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    \n",
    "    \n",
    "    # filter out short token\n",
    "    tokens = [w for w in tokens if len(w) >1]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def add_doc_to_vocab(filename,vocab):\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # update count\n",
    "    vocab.update(tokens)\n",
    "    \n",
    "def process_docs(directory, vocab):\n",
    "    # walk through all files\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if filename.startswith('cv9'):\n",
    "            continue\n",
    "        path = directory +'/'+filename\n",
    "        \n",
    "        # add doc to vocab\n",
    "        add_doc_to_vocab(path,vocab)\n",
    "        \n",
    "        \n",
    "def save_list(lines,filename):\n",
    "    # convert lines to a single blob of text\n",
    "    data = '\\n'.join(lines)\n",
    "    # open file\n",
    "    file = open(filename,'w')\n",
    "    file.write(data)\n",
    "    file.close() \n",
    "\n",
    "    \n",
    "#define vocab\n",
    "vocab = Counter()\n",
    "process_docs('/home/tri/Downloads/txt_sentoken/pos',vocab)\n",
    "process_docs('/home/tri/Downloads/txt_sentoken/neg',vocab)\n",
    "\n",
    "print(len(vocab))\n",
    "# keep tokens with a min occurences\n",
    "min_occurrence =2\n",
    "\n",
    "tokens = [k for k,c in vocab.items() if c >- min_occurrence]\n",
    "print(len(tokens))\n",
    "\n",
    "# save token to vocabulary\n",
    "save_list(tokens,'/home/tri/Downloads/vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training CNN with Embedding layer   \n",
    "\n",
    "Remind that **word embedding** illustrates text where each word in the vocabulary is represented by a real valued vector in a high dimensional space. This is better than loosing a relationship between words in the sentence as **bag-of-word**. This representation can be used while training the neural netword. Keras supports it with **Embedding** layer.   \n",
    "First load file pre_contructed **vocab.txt** with one word per line. Then, load all the training data movies review and generate a clean tokens with updated clean_doc(doc,vocab). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate clean tokens from a doc\n",
    "def clean_doc(doc, vocab):\n",
    "    # split into tokens\n",
    "    tokens =  doc.split()\n",
    "    # create regex for filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation\n",
    "    tokens = [re_punc.sub('',w) for w in tokens]\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    "\n",
    "# rewrite process_docs\n",
    "def process_docs(directory,vocab,is_train):\n",
    "    documents  = list()\n",
    "    \n",
    "    for filename in listdir(directory):\n",
    "        # skip any review in test set\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # get full path\n",
    "        path = directory +'/'+ filename\n",
    "        # load the doc\n",
    "        doc = load_doc(path)\n",
    "        # clean doc\n",
    "        tokens = clean_doc(doc,vocab)\n",
    "        # add to list\n",
    "        documents.append(tokens)\n",
    "    return documents\n",
    "\n",
    "def load_clean_dataset(vocab, is_train):\n",
    "    neg = process_docs('/home/tri/Downloads/txt_sentoken/neg',vocab,is_train)\n",
    "    pos = process_docs('/home/tri/Downloads/txt_sentoken/pos',vocab,is_train)\n",
    "    docs = neg+ pos\n",
    "    \n",
    "    labels  =np.array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
    "    return docs,labels\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to encode each document as a sequence of integer. Keras Embedding layer requires integer inputs where each integer maps to a single token that has a specific real-valued vector within th embedding. These vectors are random at the beginning of training, but get more meaningful later in training. We use API Keras Tokenizer to do this as  followings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After mapping word to integers, we use **texts_to_sequences()** function on Tokenizer. We also need all documents have the same length by padding with **pad_sequence()**. One way is to pad all reviews to the length of the longest review in the training set. This max value can be computed from the max()function: \n",
    ">max_length = max([len(s.split()) for s in train_docs])     \n",
    "The max value is used in pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create neural network model, we use Embedding as the first hidden layer. There parameters of Embedding layer are vocabulary size, the size of the real-valued vector space, and the maximum length of input documents.   \n",
    "* The **vocabulary size** is the total number of words in our vocabulary plus one for unknown word. This could be the vocab set length or the size of the vocab within tokenizer used to integer encode the document, for example:   \n",
    ">vocab_size =len(tokenizer.word_index)+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use 100 dimensional vector space but other values can be used. The maximum docment length is calculated above in the max_length variable used during padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# integer encode and pad doc\n",
    "def encode_docs(tokenizer,max_length, docs):\n",
    "    encoded = tokenizer.texts_to_sequences(docs)\n",
    "    padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
    "    return padded\n",
    "    \n",
    "\n",
    "# create model\n",
    "def create_model(vocab_size, max_length):\n",
    "    model =Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, input_length = max_length))\n",
    "    model.add(Conv1D(filters=32,kernel_size=8,activation ='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10,activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile\n",
    "    model.cmpile(loss='binary_crossentropy',optimizer='adam', metrics=['acc'])\n",
    "    # summary\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-b6ed5372877b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# calculate the maximum sequence length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_docs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Maximum length: %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "# load the vocabulary\n",
    "vocab_filename ='/home/tri/Downloads/vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab.split())\n",
    "\n",
    "# load training data\n",
    "train_docs, ytrain = load_clean_dataset(vocab,True)\n",
    "\n",
    "#create the tokenizer\n",
    "tokenizer = create_tokenizer(train_docs)\n",
    "\n",
    "# define v0cabulary size\n",
    "vocab_size = len(tokenizer.word_index)+1\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "\n",
    "# calculate the maximum sequence length\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "print('Maximum length: %d' % max_length)\n",
    "\n",
    "# encode data\n",
    "Xtrain = encode_docs(tokenizer, max_length, train_docs)\n",
    "\n",
    "\n",
    "#define model\n",
    "model = create_model(vocab_size,max_length)\n",
    "\n",
    "# fit \n",
    "model.fit(Xtrain,ytrain,epochs =10, verbose=2)\n",
    "\n",
    "# save model\n",
    "modl.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
