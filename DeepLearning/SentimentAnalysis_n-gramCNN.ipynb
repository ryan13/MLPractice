{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis is a familiar task in NLTK domain. In **TextClassification_NeuralNetwork** notebook,we explore CNN model on movie review based on **word** approach. In this, we learn **word embedding** method and show how Keras **Embedding** layer outperformes **bag-of-word** approach. In this notebook we study n-gram CNN model for sentiment analysis.   \n",
    "In keras, a multiple  inpur model can be defined using function API. We will define a model with three input channels for processing 4 grams, 6-grams and 8-grams of movie review test. Each channel is comprised of the following element   \n",
    "* **Input** layer defines the length of sequences\n",
    "* **Embedding** layer set to the size of vocabulary and 100-dimensional real-valued representation   \n",
    "* **Conv1D** layer with 32 filters and a kernel size set to the number of words ro read at once.   \n",
    "* **MaxPooling1D** layer to consolidate the output from the convolutional layer.   \n",
    "* **Flatten** layer to reduce the three-dimensional outputto two dimenstional for concatenation.   \n",
    "\n",
    "The output from the three channels are concatenated into a single vector and process by a **Dense** layer and an output layer. This can be done as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(length, vocab_size):\n",
    "    # channel 1\n",
    "    inputs1 = Input(shape=(length,))\n",
    "    embedding1 = Embedding(vocab_size,100)(inputs1)\n",
    "    conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
    "    drop1 = Dropout(0.5)(conv1)\n",
    "    pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "    \n",
    "    # channel 2\n",
    "    inputs2 = Input(shape=(length,))\n",
    "    embedding2 = Embedding(vocab_size,100)(inputs2)\n",
    "    conv2 = Conv1D(filters=32,kernel_size=6, activation ='relu')(embedding2)\n",
    "    drop2 = Dropout(0.5)(conv2)\n",
    "    pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "    flat2 = Flatten()(pool2)\n",
    "    \n",
    "    # channel 3\n",
    "    inputs3 = Input(shape=(length,))\n",
    "    embedding3 = Embedding(vocab_size,100)(inputs3)\n",
    "    conv3 = Conv1D(filters=32, kernel_size =8, activation='relu')(embedding3)\n",
    "    drop3 = Dropout(0.5)(conv3)\n",
    "    pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "    flat3 = Flatten()(pool3)\n",
    "    \n",
    "    # merge\n",
    "    merged = concatenate([flat1,flat2, flat3])\n",
    "    \n",
    "    # intepretation\n",
    "    dense1 = Dense(10, activation='relu')(merged)\n",
    "    outputs = Dense(1, activation='sigmoid')(dense1)\n",
    "    model = Model(inputs=[inputs1,inputs2,inputs3],outputs=outputs)\n",
    "    \n",
    "    # compile\n",
    "    model.compile(loss='binary_crossentropy', optimizer ='adam',metrics =['accuracy'])\n",
    "    # summary\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remain code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load,dump\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from os import listdir\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, Embedding\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers.merge import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: train.pkl\n",
      "Saved: test.pkl\n"
     ]
    }
   ],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    file = open(filename,'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "#generate clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into token\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char\n",
    "    re_punc = re.compile('[%s]'% re.escape(string.punctuation))\n",
    "    # remove punctuation\n",
    "    tokens = [re_punc.sub('',w) for w in tokens]\n",
    "    # eleminate non alphabetic\n",
    "    tokens = [w for w in tokens if w.isalpha()]\n",
    "    \n",
    "    # filter out stop word\n",
    "    stop_words =set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    \n",
    "    # filter out short tokens\n",
    "    tokens = [w for w in tokens if len(w) > 1]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, is_train):\n",
    "    documents = list()\n",
    "    # walk through all files\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        path = directory +'/' + filename\n",
    "        doc = load_doc(path)\n",
    "    # clean doc\n",
    "        tokens = clean_doc(doc)\n",
    "    # add to list\n",
    "        documents.append(tokens)\n",
    "    return documents\n",
    "\n",
    "def load_clean_dataset(is_train):\n",
    "# load and clean a dataset\n",
    "    neg = process_docs('/home/tri/Downloads/txt_sentoken/neg',is_train)\n",
    "    pos = process_docs('/home/tri/Downloads/txt_sentoken/pos',is_train)\n",
    "    docs = neg +pos\n",
    "    \n",
    "    labels =[0 for _ in range(len(neg))] +[1 for _ in range(len(pos))]\n",
    "    return docs, labels\n",
    " \n",
    "def save_dataset(dataset, filename):\n",
    "    dump(dataset, open(filename,'wb'))\n",
    "    print('Saved: %s' % filename)\n",
    "    \n",
    "    \n",
    "train_docs , ytrain = load_clean_dataset(True)\n",
    "test_docs,ytest = load_clean_dataset(False)\n",
    "\n",
    "# save training datasets\n",
    "\n",
    "save_dataset([train_docs,ytrain],'train.pkl')\n",
    "save_dataset([test_docs,ytest],'test.pkl')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a clean dataset\n",
    "def load_dataset(filename):\n",
    "    return load(open(filename,'rb'))\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# calculate the maximum document length\n",
    "def max_length(lines):\n",
    "    return max([len(s.split()) for s in lines])\n",
    "\n",
    "\n",
    "# encoding a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    # padding encoded sequences\n",
    "    padded = pad_sequences(encoded, maxlen= length, padding='post')\n",
    "    \n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 1380\n",
      "Vocabulary size: 44277\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_11 (InputLayer)            (None, 1380)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_12 (InputLayer)            (None, 1380)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_13 (InputLayer)            (None, 1380)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)         (None, 1380, 100)     4427700     input_11[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)         (None, 1380, 100)     4427700     input_12[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)         (None, 1380, 100)     4427700     input_13[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)               (None, 1377, 32)      12832       embedding_11[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)               (None, 1375, 32)      19232       embedding_12[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)               (None, 1373, 32)      25632       embedding_13[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)             (None, 1377, 32)      0           conv1d_11[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)             (None, 1375, 32)      0           conv1d_12[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)             (None, 1373, 32)      0           conv1d_13[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D)  (None, 688, 32)       0           dropout_11[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D)  (None, 687, 32)       0           dropout_12[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D)  (None, 686, 32)       0           dropout_13[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)             (None, 22016)         0           max_pooling1d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)             (None, 21984)         0           max_pooling1d_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)             (None, 21952)         0           max_pooling1d_13[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)      (None, 65952)         0           flatten_11[0][0]                 \n",
      "                                                                   flatten_12[0][0]                 \n",
      "                                                                   flatten_13[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 10)            659530      concatenate_4[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 1)             11          dense_7[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 14,000,337\n",
      "Trainable params: 14,000,337\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Epoch 1/7\n",
      "1800/1800 [==============================] - 137s - loss: 0.6991 - acc: 0.4878   \n",
      "Epoch 2/7\n",
      "1800/1800 [==============================] - 135s - loss: 0.6932 - acc: 0.4822   \n",
      "Epoch 3/7\n",
      "1800/1800 [==============================] - 159s - loss: 0.6932 - acc: 0.5000   \n",
      "Epoch 4/7\n",
      "1800/1800 [==============================] - 125s - loss: 0.6932 - acc: 0.5000   \n",
      "Epoch 5/7\n",
      "1800/1800 [==============================] - 140s - loss: 0.6932 - acc: 0.4856   \n",
      "Epoch 6/7\n",
      "1800/1800 [==============================] - 157s - loss: 0.6932 - acc: 0.4856   \n",
      "Epoch 7/7\n",
      " 848/1800 [=============>................] - ETA: 70s - loss: 0.6932 - acc: 0.4976"
     ]
    }
   ],
   "source": [
    "# load training\n",
    "trainLines, trainLabels = load_dataset('train.pkl')\n",
    "\n",
    "# create tokenizer\n",
    "tokenizer = create_tokenizer(trainLines)\n",
    "\n",
    "# calculate max doc length\n",
    "length= max_length(trainLines)\n",
    "print('Max document length: %d' % length)\n",
    "\n",
    "#calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary size: %d' %vocab_size)\n",
    "\n",
    "# encode data\n",
    "trainX = encode_text(tokenizer, trainLines, length)\n",
    "\n",
    "# create model\n",
    "model = create_model(length, vocab_size)\n",
    "\n",
    "# fit model\n",
    "model.fit([trainX, trainX, trainX], np.array(trainLabels), epochs=7, batch_size =16)\n",
    "\n",
    "# save model\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
