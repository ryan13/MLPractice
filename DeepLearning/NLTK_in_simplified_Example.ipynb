{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'morn', ',', 'when', 'gregor', 'samsa', 'woke', 'from', 'troubl', 'dream', ',', 'he', 'found', 'himself', 'transform', 'in', 'hi', 'bed', 'into', 'a', 'horribl', 'vermin', '.', 'He', 'lay', 'on', 'hi', 'armour-lik', 'back', ',', 'and', 'if', 'he', 'lift', 'hi', 'head', 'a', 'littl', 'he', 'could', 'see', 'hi', 'brown', 'belli', ',', 'slightli', 'dome', 'and', 'divid', 'by', 'arch', 'into', 'stiff', 'section', '.', 'the', 'bed', 'wa', 'hardli', 'abl', 'to', 'cover', 'it', 'and', 'seem', 'readi', 'to', 'slide', 'off', 'ani', 'moment', '.', 'hi', 'mani', 'leg', ',', 'piti', 'thin', 'compar', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', ',', 'wave', 'about', 'helplessli', 'as', 'he', 'look', '.', '``', 'what', \"'s\", 'happen', 'to']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "filename = '/home/tri/Downloads/MLdatasets/metamorphosis_clean.txt'\n",
    "file = open(filename,'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "tokens = word_tokenize(text)\n",
    "porter = PorterStemmer()\n",
    "stemmed =[porter.stem(word) for word in tokens]\n",
    "print(stemmed[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'brown': 0, 'the': 7, 'fox': 2, 'quick': 6, 'lazy': 4, 'over': 5, 'dog': 1, 'jumped': 3}\n",
      "(1, 8)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[1 1 1 1 1 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "# Transform\n",
    "vectorizer = CountVectorizer()\n",
    "# Tokenize\n",
    "vectorizer.fit(text)\n",
    "print(vectorizer.vocabulary_)\n",
    "# Encode document\n",
    "vector = vectorizer.transform(text)\n",
    "\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Example of TfidfVectorizer\n",
    "\n",
    "Simply word count may be not useful when a word can appear multiple time but less informative. TfidfVectorizer deals with this problem by considering term frequency and Inverse Document Frequency. In fact, **TfidfVectorizer** equals to both **CountVectorizer** and **TfidfTransformer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'brown': 0, 'the': 7, 'fox': 2, 'quick': 6, 'lazy': 4, 'over': 5, 'dog': 1, 'jumped': 3}\n",
      "[ 1.69314718  1.28768207  1.28768207  1.69314718  1.69314718  1.69314718\n",
      "  1.69314718  1.        ]\n",
      "(1, 8)\n",
      "[[ 0.36388646  0.27674503  0.27674503  0.36388646  0.36388646  0.36388646\n",
      "   0.36388646  0.42983441]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\",\"The dog.\",\"The fox\"]\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(text)\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.idf_)\n",
    "\n",
    "\n",
    "vector = vectorizer.transform([text[0]])\n",
    "print(vector.shape)\n",
    "print(vector.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashing with HashingVectorizer\n",
    "Count and frequencies have a limitation when vocabulary become large. One approach is **hashing** that converts text into number using fixed length vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20)\n",
      "[[ 0.          0.          0.          0.          0.          0.33333333\n",
      "   0.         -0.33333333  0.33333333  0.          0.          0.33333333\n",
      "   0.          0.          0.         -0.33333333  0.          0.\n",
      "  -0.66666667  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "text= ['The quick brown fox jumped over the lazy dog.']\n",
    "vectorizer = HashingVectorizer(n_features=20)\n",
    "\n",
    "vector = vectorizer.transform(text)\n",
    "\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text operation with Keras   \n",
    "Instead of nltk functions, keras provides similar methods. In the following example, we increase the vcabulary size by one-third to minimize collision when hashing words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_word_sequence(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 5, 6, 6, 7, 7, 7, 7]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import one_hot, text_to_word_sequence\n",
    "\n",
    "text = \"The quick brown fox jumpedover the lazy dog.\"   # note that there is no bracket\n",
    "\n",
    "# estimate the size of the vocabulary\n",
    "words = set(text_to_word_sequence(text))\n",
    "vocab_size = len(words)\n",
    "\n",
    "result =one_hot(text,n=round(vocab_size*1.3),lower=True)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashing trick from keras   \n",
    "Count-based encoding approach requires to maintain vocabulary of words and their mapping to integer. Aternative, a one-way hash function avoids to keep track of a vocabulary to convert words to integer. Keras provide **hashing trick** to tokenize and numeric encoding similar to **one_hot** function. More importance, **hashing_trick** function from keras allows either **hash** or other hash functions such as **md5**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'brown', 'dog', 'fox', 'jumped', 'lazy', 'over', 'quick', 'the'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import hashing_trick\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "text = 'The quick brown fox jumped over the lazy dog.'\n",
    "set(text_to_word_sequence(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 4, 1, 2, 7, 5, 6, 2, 6]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import hashing_trick\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "text = 'The quick brown fox jumped over the lazy dog.'\n",
    "\n",
    "words =set(text_to_word_sequence(text))\n",
    "vocab_size = len(words)\n",
    "\n",
    "result = hashing_trick(text, round(vocab_size*1.3), hash_function='md5')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Tokenizer in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('well', 1), ('done', 1), ('good', 1), ('work', 2), ('great', 1), ('effort', 1), ('nice', 1), ('excellent', 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "docs = ['Well done!','Good work','Great effort','nice work','Excellent!']\n",
    "\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "print(t.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(t.document_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'well': 2, 'effort': 6, 'excellent': 8, 'good': 4, 'great': 5, 'done': 3, 'nice': 7, 'work': 1}\n"
     ]
    }
   ],
   "source": [
    "print(t.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'done': 1, 'effort': 1, 'excellent': 1, 'good': 1, 'great': 1, 'well': 1, 'nice': 1, 'work': 2}\n"
     ]
    }
   ],
   "source": [
    "print(t.word_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  1.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "encoded_docs = t.texts_to_matrix(docs, mode='count')\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Word   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from os import listdir\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    file = open(filename,'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def clean_doc(doc):\n",
    "    tokens = doc.split()\n",
    "    re_punc = re.compile('[%s]'% re.escape(string.punctuation))\n",
    "    \n",
    "    tokens =[re_punc.sub('',w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens  = [w for w in tokens if not w in stop_words ]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) >1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename,vocab):\n",
    "    doc = load_doc(filename)\n",
    "    tokens = clean_doc(doc)\n",
    "    vocab.update(tokens)\n",
    "    \n",
    "def process_docs(directory,vocab):\n",
    "    # walk through all files\n",
    "    for filename in listdir(directory):\n",
    "        if filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path \n",
    "        path = directory + '/' + filename\n",
    "        # add doc to vocab\n",
    "        add_doc_to_vocab(path, vocab)\n",
    "        \n",
    "def save_list(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename,'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44276\n"
     ]
    }
   ],
   "source": [
    "vocab = Counter()\n",
    "\n",
    "process_docs('/home/tri/Downloads/txt_sentoken/pos',vocab)\n",
    "process_docs('/home/tri/Downloads/txt_sentoken/neg',vocab)\n",
    "\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25767\n"
     ]
    }
   ],
   "source": [
    "min_occurence = 2\n",
    "tokens =[k for k,c in vocab.items() if c >= min_occurence]\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_list(tokens,'/home/tri/Downloads/vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refine and adding useful function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doc_to_line(filename, vocab):\n",
    "    doc = load_doc(filename)\n",
    "    tokens = clean_doc(doc)\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Modify process_docs\n",
    "def process_docs(directory, vocab, is_train):\n",
    "    lines = list()\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "            \n",
    "        # create the full path of the file to open    \n",
    "        \n",
    "        path = directory +'/'+ filename\n",
    "        # load and clean the doc\n",
    "        line = doc_to_line(path,vocab)\n",
    "        # add to list\n",
    "        lines.append(line)\n",
    "    return lines\n",
    "\n",
    "def load_clean_dataset(vocab, is_train):\n",
    "    neg = process_docs('/home/tri/Downloads/txt_sentoken/neg',vocab, is_train)\n",
    "    pos = process_docs('/home/tri/Downloads/txt_sentoken/pos',vocab, is_train)\n",
    "    docs = neg+ pos\n",
    "    # prepare label\n",
    "    labels = [0 for _ in range(len(neg))] +[1 for _ in range(len(pos))]\n",
    "    return docs, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 25768) (0, 25768)\n"
     ]
    }
   ],
   "source": [
    "# Redo the process, load all reviews\n",
    "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
    "test_docs, ytest = load_clean_dataset(vocab, False)\n",
    "\n",
    "tokenizer = create_tokenizer(train_docs)\n",
    "# encoding data\n",
    "\n",
    "Xtrain = tokenizer.texts_to_matrix(train_docs,mode = 'freq')\n",
    "Xtest = tokenizer.texts_to_matrix(test_docs, mode='freq')\n",
    "\n",
    "print(Xtrain.shape, Xtest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the first sentiment analysis model   \n",
    "Here we include full code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    file= open(filename,'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def clean_doc(doc):\n",
    "    tokens =doc.split()\n",
    "    re_punc = re.compile('[%s]'% re.escape(string.punctuation))\n",
    "    # Remove punctuation \n",
    "    tokens = [re_punc.sub('',w) for w in tokens]\n",
    "    \n",
    "    # Remoeve remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    \n",
    "    # filter out stopword\n",
    "    stop_words =set(stopwords.words('english'))\n",
    "    tokens =[w for w in tokens if not w in stop_words]\n",
    "    \n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word)>1]\n",
    "    return tokens\n",
    "\n",
    "# load doc, clean and generate line of tokens\n",
    "def doc_to_line(filename, vocab):\n",
    "    doc = load_doc(filename)\n",
    "    tokens = clean_doc(doc)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens) \n",
    "\n",
    "# load all docs from directory\n",
    "def process_docs(directory, vocab, is_train):\n",
    "    lines = list()\n",
    "    # walk through all files\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        \n",
    "        path = directory +'/'+filename\n",
    "        # load and clean\n",
    "        line = doc_to_line(path,vocab)\n",
    "        lines.append(line)\n",
    "    return lines\n",
    "\n",
    "# load and clean dataset\n",
    "def load_clean_dataset(vocab, is_train):\n",
    "    neg = process_docs('/home/tri/Downloads/txt_sentoken/neg',vocab, is_train)\n",
    "    pos = process_docs('/home/tri/Downloads/txt_sentoken/pos',vocab,is_train)\n",
    "    docs = neg+pos\n",
    "    labels = np.array([0 for _ in range(len(neg))] +[1 for _ in range(len(pos))])\n",
    "    return docs,labels\n",
    "    \n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# create model\n",
    "def create_model(n_words):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50,input_shape=(n_words,),activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test accuracy 87.500000\n"
     ]
    }
   ],
   "source": [
    "vocab_filename = '/home/tri/Downloads/vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab.split())\n",
    "\n",
    "# load reviews\n",
    "train_docs , ytrain = load_clean_dataset(vocab, True)\n",
    "test_docs, ytest = load_clean_dataset(vocab,False)\n",
    "# create tokenizer\n",
    "tokenizer =  create_tokenizer(train_docs)\n",
    "# encoding\n",
    "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='freq')\n",
    "Xtest  = tokenizer.texts_to_matrix(test_docs,mode='freq')\n",
    "\n",
    "n_words = Xtest.shape[1]\n",
    "model= create_model(n_words)\n",
    "\n",
    "model.fit(Xtrain,ytrain,epochs =10,verbose=0)\n",
    "\n",
    "# Evaluation\n",
    "loss, acc = model.evaluate(Xtest, ytest, verbose =0)\n",
    "print('Test accuracy %f' %(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing word scoring methods\n",
    "The **text_to_matrix()** function for the Tokenizer in Keras API provides 4 different methods for scoring words:\n",
    "* **binary** Where words are marked as present (1) or above (0)\n",
    "* **count** Where occurence count for each word is marked s an integer\n",
    "* **tfidf** Where each word is scored based on their frequency s.t more frequent words will be penalized.   \n",
    "* **freq**: words are scored based on their frequenct of occurence within document.   \n",
    "\n",
    "Let experiment the above model with different scoring by building **prepare_data()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare bag-of-word encoding of docs\n",
    "def prepare_data(train_docs, test_docs,mode):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(train_docs)\n",
    "    # encoding training data set\n",
    "    Xtrain = tokenizer.texts_to_matrix(train_docs, mode=mode)\n",
    "    # encoding testing \n",
    "    Xtest  = tokenizer.texts_to_matrix(test_docs, mode=mode)\n",
    "    return Xtrain, Xtest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Neural network models is schochastic, they can genereate different results due to initial random weights and shuffling of pattern during mini-batch gradiant descent. As such we will introduce the **evaluate_mode** function to evaluate the performance on different test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mode(X_train,ytrain,Xtest,ytest):\n",
    "    scores = list()\n",
    "    n_repeats = 30\n",
    "    n_words = Xtest.shape[1]\n",
    "    for i in range(n_repeats):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(50,input_shape=(n_words,),activation='relu'))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "        model.fit(Xtrain,ytrain,epochs=10, verbose=0)\n",
    "        # evaluate\n",
    "        loss, acc =model.evaluate(Xtest,ytest,verbose=0)\n",
    "        scores.append(acc)\n",
    "        print('%d accuracy: %s' % ((i+1), acc))\n",
    "    return scores   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 accuracy: 0.93\n",
      "2 accuracy: 0.92\n",
      "3 accuracy: 0.935\n",
      "4 accuracy: 0.925\n",
      "5 accuracy: 0.94\n",
      "6 accuracy: 0.925\n",
      "7 accuracy: 0.925\n",
      "8 accuracy: 0.935\n",
      "9 accuracy: 0.93\n",
      "10 accuracy: 0.915\n",
      "11 accuracy: 0.925\n",
      "12 accuracy: 0.915\n",
      "13 accuracy: 0.925\n",
      "14 accuracy: 0.925\n",
      "15 accuracy: 0.93\n",
      "16 accuracy: 0.92\n",
      "17 accuracy: 0.935\n",
      "18 accuracy: 0.935\n",
      "19 accuracy: 0.925\n",
      "20 accuracy: 0.915\n",
      "21 accuracy: 0.925\n",
      "22 accuracy: 0.935\n",
      "23 accuracy: 0.93\n",
      "24 accuracy: 0.925\n",
      "25 accuracy: 0.93\n",
      "26 accuracy: 0.93\n",
      "27 accuracy: 0.925\n",
      "28 accuracy: 0.93\n",
      "29 accuracy: 0.935\n",
      "30 accuracy: 0.93\n",
      "1 accuracy: 0.895\n",
      "2 accuracy: 0.9\n",
      "3 accuracy: 0.905\n",
      "4 accuracy: 0.885\n",
      "5 accuracy: 0.89\n",
      "6 accuracy: 0.91\n",
      "7 accuracy: 0.91\n",
      "8 accuracy: 0.9\n",
      "9 accuracy: 0.89\n",
      "10 accuracy: 0.895\n",
      "11 accuracy: 0.905\n",
      "12 accuracy: 0.91\n",
      "13 accuracy: 0.91\n",
      "14 accuracy: 0.905\n",
      "15 accuracy: 0.9\n",
      "16 accuracy: 0.895\n",
      "17 accuracy: 0.905\n",
      "18 accuracy: 0.89\n",
      "19 accuracy: 0.91\n",
      "20 accuracy: 0.895\n",
      "21 accuracy: 0.9\n",
      "22 accuracy: 0.895\n",
      "23 accuracy: 0.895\n",
      "24 accuracy: 0.9\n",
      "25 accuracy: 0.895\n",
      "26 accuracy: 0.91\n",
      "27 accuracy: 0.895\n",
      "28 accuracy: 0.91\n",
      "29 accuracy: 0.885\n",
      "30 accuracy: 0.9\n",
      "1 accuracy: 0.87\n",
      "2 accuracy: 0.855\n",
      "3 accuracy: 0.9\n",
      "4 accuracy: 0.845\n",
      "5 accuracy: 0.88\n",
      "6 accuracy: 0.885\n",
      "7 accuracy: 0.885\n",
      "8 accuracy: 0.875\n",
      "9 accuracy: 0.895\n",
      "10 accuracy: 0.855\n",
      "11 accuracy: 0.885\n",
      "12 accuracy: 0.855\n",
      "13 accuracy: 0.885\n",
      "14 accuracy: 0.88\n",
      "15 accuracy: 0.85\n",
      "16 accuracy: 0.88\n",
      "17 accuracy: 0.885\n",
      "18 accuracy: 0.865\n",
      "19 accuracy: 0.895\n",
      "20 accuracy: 0.89\n",
      "21 accuracy: 0.86\n",
      "22 accuracy: 0.875\n",
      "23 accuracy: 0.885\n",
      "24 accuracy: 0.875\n",
      "25 accuracy: 0.865\n",
      "26 accuracy: 0.86\n",
      "27 accuracy: 0.895\n",
      "28 accuracy: 0.875\n",
      "29 accuracy: 0.88\n",
      "30 accuracy: 0.875\n",
      "1 accuracy: 0.86\n",
      "2 accuracy: 0.86\n",
      "3 accuracy: 0.875\n",
      "4 accuracy: 0.87\n",
      "5 accuracy: 0.865\n",
      "6 accuracy: 0.87\n",
      "7 accuracy: 0.87\n",
      "8 accuracy: 0.88\n",
      "9 accuracy: 0.885\n",
      "10 accuracy: 0.87\n",
      "11 accuracy: 0.86\n",
      "12 accuracy: 0.865\n",
      "13 accuracy: 0.875\n",
      "14 accuracy: 0.865\n",
      "15 accuracy: 0.87\n",
      "16 accuracy: 0.865\n",
      "17 accuracy: 0.875\n",
      "18 accuracy: 0.86\n",
      "19 accuracy: 0.88\n",
      "20 accuracy: 0.87\n",
      "21 accuracy: 0.87\n",
      "22 accuracy: 0.87\n",
      "23 accuracy: 0.88\n",
      "24 accuracy: 0.865\n",
      "25 accuracy: 0.865\n",
      "26 accuracy: 0.87\n",
      "27 accuracy: 0.87\n",
      "28 accuracy: 0.87\n",
      "29 accuracy: 0.865\n",
      "30 accuracy: 0.87\n",
      "          binary      count      tfidf       freq\n",
      "count  30.000000  30.000000  30.000000  30.000000\n",
      "mean    0.927500   0.899667   0.875333   0.869500\n",
      "std     0.006399   0.007761   0.014559   0.006345\n",
      "min     0.915000   0.885000   0.845000   0.860000\n",
      "25%     0.925000   0.895000   0.865000   0.865000\n",
      "50%     0.927500   0.900000   0.877500   0.870000\n",
      "75%     0.930000   0.905000   0.885000   0.870000\n",
      "max     0.940000   0.910000   0.900000   0.885000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-2ce978b3a9fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboxplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# create tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# remaining program\n",
    "vocab_filename = '/home/tri/Downloads/vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab.split())\n",
    "\n",
    "# load reviews\n",
    "train_docs , ytrain = load_clean_dataset(vocab, True)\n",
    "test_docs, ytest = load_clean_dataset(vocab,False)\n",
    "\n",
    "\n",
    "modes = ['binary','count','tfidf','freq']\n",
    "results = pd.DataFrame()\n",
    "\n",
    "for mode in modes:\n",
    "    Xtrain, Xtest = prepare_data(train_docs, test_docs,mode)\n",
    "    # evaluate\n",
    "    results[mode] = evaluate_mode(Xtrain, ytrain, Xtest, ytest)\n",
    "# summarize\n",
    "print(results.describe())\n",
    "\n",
    "results.boxplot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting sentiment for new Reviews   \n",
    "Finally, we can develop a final model to make prediction for new reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(review, vocab, tokenizer, model):\n",
    "    tokens =clean_doc(review)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    line = ' '.join(tokens)\n",
    "    # encoding\n",
    "    encoded = tokenizer.texts_to_matrix([line], mode='binary')\n",
    "    # predict sentiment\n",
    "    yhat = model.predict(encoded, verbose=0)\n",
    "    # retrieve predicted percentage and label\n",
    "    percent_pos = yhat[0,0]\n",
    "    \n",
    "    if round(percent_pos) ==0:\n",
    "        return (1 - percent_pos), 'NEGATIVE'\n",
    "    return percent_pos, 'POSITIVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: [Best movie ever! It was greate, I recommend it.]\n",
      "Sentiment: NEGATIVE (83.057%)\n",
      "Review: [This is a bad movie.]\n",
      "Sentiment: NEGATIVE (100.000%)\n"
     ]
    }
   ],
   "source": [
    "text = 'Best movie ever! It was greate, I recommend it.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' %(text,sentiment,percent*100))\n",
    "# test negative text\n",
    "text ='This is a bad movie.'\n",
    "percent, sentiment = predict_sentiment(text,vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)'% (text, sentiment,percent *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
